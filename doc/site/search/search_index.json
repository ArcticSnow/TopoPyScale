{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"0_index/","text":"Welcome to TopoPyScale Documentation Image caption {align=right} TopoPyScale is a downscaling toolbox for global and regional climate model datasets, particularly relevant to mountain ranges, and hillslopes. Source Code Github Repository : https://github.com/ArcticSnow/TopoPyScale Examples Repository : https://github.com/ArcticSnow/TopoPyScale_examples Documentation Repository : h ttps://github.com/ArcticSnow/TopoPyScale_Documentation If you are here to use TopoPyScale, then head to the Quick Start page. Further configuration setup are explained in detail General Concept TopoPyScale uses both climate model/reanalysis data and Digital Elevation Models (DEM) for correcting atmospheric state variables (e.g. temperature, pressure, humidity, etc ). TopoPyScale provides tools to interpolate and correct such variables to be relevant locally given a topographical context. (descirbe why we need the DEM) The most basic requirements of TopoPyscale is a DEM used to defined the spatial domain of interest as well as compute a number of morphometrics, and configuration file defining the temporal period, the downscaling methods and other parameters. In its current version, TopoPyScale includes the topoclass class that wraps all functionalities for ease of use. It automatically fetches data from the ERA5 repositories (Pressure and Surface levels). Other climate data sources can be added. Based on the high resolution (30-100m) DEM and the climate data, methods in the topoclass will compute, correct and interpolate variables need to force specialized land-surface models. TopoPyScale includes a number of export format inter-operable with specialized energy and mass balance land surface models like CRYOGRID , CROCUS , SNOWPACK , FSM , Snowmodel , GEOTOP or MuSa . Downscaled variable includes: 2m air temperature 2m air humidity 2m air pressure 10m wind speed and direction Surface shortwave incoming radiation Surface longwave incoming radiation Precipitation (possibility to partition snow and rain) (insert sketches and diagram to illustrate the principle) Quick Installation Release Installation To install the latest release, in a virtual environment simply use pip pip install topopyscale As of now, TopoPyScale uses the Copernicus cdsapi to download data. For this to work, you will need to setup the Copernicus API key in your system. Follow this tutorial after creating an account with Copernicus. On Linux, create a file nano ~/.cdsapirc with inside: url: https://cds.climate.copernicus.eu/api/v2 key: {uid}:{api-key} Funding and Support TopoPyScale is currently developed by scientists at: University of Oslo , Norway SLF, Switzerland","title":"Home"},{"location":"0_index/#welcome-to-topopyscale-documentation","text":"Image caption {align=right} TopoPyScale is a downscaling toolbox for global and regional climate model datasets, particularly relevant to mountain ranges, and hillslopes. Source Code Github Repository : https://github.com/ArcticSnow/TopoPyScale Examples Repository : https://github.com/ArcticSnow/TopoPyScale_examples Documentation Repository : h ttps://github.com/ArcticSnow/TopoPyScale_Documentation If you are here to use TopoPyScale, then head to the Quick Start page. Further configuration setup are explained in detail","title":"Welcome to TopoPyScale Documentation"},{"location":"0_index/#general-concept","text":"TopoPyScale uses both climate model/reanalysis data and Digital Elevation Models (DEM) for correcting atmospheric state variables (e.g. temperature, pressure, humidity, etc ). TopoPyScale provides tools to interpolate and correct such variables to be relevant locally given a topographical context. (descirbe why we need the DEM) The most basic requirements of TopoPyscale is a DEM used to defined the spatial domain of interest as well as compute a number of morphometrics, and configuration file defining the temporal period, the downscaling methods and other parameters. In its current version, TopoPyScale includes the topoclass class that wraps all functionalities for ease of use. It automatically fetches data from the ERA5 repositories (Pressure and Surface levels). Other climate data sources can be added. Based on the high resolution (30-100m) DEM and the climate data, methods in the topoclass will compute, correct and interpolate variables need to force specialized land-surface models. TopoPyScale includes a number of export format inter-operable with specialized energy and mass balance land surface models like CRYOGRID , CROCUS , SNOWPACK , FSM , Snowmodel , GEOTOP or MuSa . Downscaled variable includes: 2m air temperature 2m air humidity 2m air pressure 10m wind speed and direction Surface shortwave incoming radiation Surface longwave incoming radiation Precipitation (possibility to partition snow and rain) (insert sketches and diagram to illustrate the principle)","title":"General Concept"},{"location":"0_index/#quick-installation","text":"","title":"Quick Installation"},{"location":"0_index/#release-installation","text":"To install the latest release, in a virtual environment simply use pip pip install topopyscale As of now, TopoPyScale uses the Copernicus cdsapi to download data. For this to work, you will need to setup the Copernicus API key in your system. Follow this tutorial after creating an account with Copernicus. On Linux, create a file nano ~/.cdsapirc with inside: url: https://cds.climate.copernicus.eu/api/v2 key: {uid}:{api-key}","title":"Release Installation"},{"location":"0_index/#funding-and-support","text":"TopoPyScale is currently developed by scientists at: University of Oslo , Norway SLF, Switzerland","title":"Funding and Support"},{"location":"1_instal/","text":"Installation Release Installation To install the latest release, in a virtual environment simply use pip pip install topopyscale Development version Installation To install the version in development: - [x] tested on Linux Ubuntu - [ ] tested on MacOS conda install mamba -n base -c conda-forge mamba create -n downscaling ipython numpy pandas xarray matplotlib netcdf4 ipykernel scikit-learn rasterio gdal pyproj munch conda activate downscaling pip install cdsapi pip install h5netcdf pip install topocalc pip install pvlib pip install elevation pip install lazydocs cd github # navigate to where you want to clone TopoPyScale git clone git@github.com:ArcticSnow/TopoPyScale.git pip install -e TopoPyScale #install a development version, remove the -e for normal install #---------------------------------------------------------- # OPTIONAL: if using jupyter lab # add this new Python kernel to your jupyter lab PATH python -m ipykernel install --user --name downscaling Setting up cdsapi Then you need to setup your cdsapi with the Copernicus API key system. Follow this tutorial after creating an account with Copernicus . On Linux, create a file nano ~/.cdsapirc with inside: url: https://cds.climate.copernicus.eu/api/v2 key: {uid}:{api-key}","title":"Installation"},{"location":"1_instal/#installation","text":"","title":"Installation"},{"location":"1_instal/#release-installation","text":"To install the latest release, in a virtual environment simply use pip pip install topopyscale","title":"Release Installation"},{"location":"1_instal/#development-version-installation","text":"To install the version in development: - [x] tested on Linux Ubuntu - [ ] tested on MacOS conda install mamba -n base -c conda-forge mamba create -n downscaling ipython numpy pandas xarray matplotlib netcdf4 ipykernel scikit-learn rasterio gdal pyproj munch conda activate downscaling pip install cdsapi pip install h5netcdf pip install topocalc pip install pvlib pip install elevation pip install lazydocs cd github # navigate to where you want to clone TopoPyScale git clone git@github.com:ArcticSnow/TopoPyScale.git pip install -e TopoPyScale #install a development version, remove the -e for normal install #---------------------------------------------------------- # OPTIONAL: if using jupyter lab # add this new Python kernel to your jupyter lab PATH python -m ipykernel install --user --name downscaling","title":"Development version Installation"},{"location":"1_instal/#setting-up-cdsapi","text":"Then you need to setup your cdsapi with the Copernicus API key system. Follow this tutorial after creating an account with Copernicus . On Linux, create a file nano ~/.cdsapirc with inside: url: https://cds.climate.copernicus.eu/api/v2 key: {uid}:{api-key}","title":"Setting up cdsapi"},{"location":"2_quickstart/","text":"Quick Start Basic usage Setup your Python environment Create your project directory Configure the file config.yml to fit your problem (see config.yml for an example) Run TopoPyScale import pandas as pd from TopoPyScale import topoclass as tc from matplotlib import pyplot as plt # ========= STEP 1 ========== # Load Configuration config_file = './config.yml' mp = tc . Topoclass ( config_file ) # ======== STEP 2 =========== # Compute parameters of the DEM (slope, aspect, sky view factor) mp . compute_dem_param () mp . extract_topo_param () # ========= STEP 3 ========== # compute solar geometry and horizon angles mp . compute_solar_geometry () mp . compute_horizon () # ========= STEP 4 ========== # Perform the downscaling mp . downscale_climate () # ========= STEP 5 ========== # explore the downscaled dataset. For instance the temperature difference between each point and the first one ( mp . downscaled_pts . t - mp . downscaled_pts . t . isel ( point_id = 0 )) . plot () plt . show () # ========= STEP 6 ========== # Export output to desired format mp . to_netcdf () TopoClass will create a file structure in the project folder (see below). TopoPyScale assumes you have a DEM in GeoTiFF, and a set of climate data in netcdf (following ERA5 variable conventions). TopoPyScale can easier segment the DEM using clustering (e.g. K-mean), or a list of predefined point coordinates in pts_list.csv can be provided. Make sure all parameters in config.yml are correct. my_project/ \u251c\u2500\u2500 inputs/ \u251c\u2500\u2500 dem/ \u251c\u2500\u2500 my_dem.tif \u2514\u2500\u2500 pts_list.csv (optional) \u2514\u2500\u2500 climate/ \u251c\u2500\u2500 PLEV*.nc \u2514\u2500\u2500 SURF*.nc \u251c\u2500\u2500 outputs/ \u251c\u2500\u2500 tmp/ \u2514\u2500\u2500 config.yml Plotting TopoPyScale incudes a number of plotting tools: # To plot cluster map: mp . toposub . plot_clusters_map () # To plot sky view factor or any other variable mp . toposub . plot_clusters_map ( var = 'svf' , cmap = plt . cm . viridis ) Comparison to Observations TopoPyScale includes tools to fetch weather station observations from public databases: WMO Norwegian meteoroligical Institue MetNO FROST API These functions are available into topo_obs.py . topo_compare.py includes functions to estimate and also correct bias between downscaled and a reference timeseries.","title":"Quick Start"},{"location":"2_quickstart/#quick-start","text":"","title":"Quick Start"},{"location":"2_quickstart/#basic-usage","text":"Setup your Python environment Create your project directory Configure the file config.yml to fit your problem (see config.yml for an example) Run TopoPyScale import pandas as pd from TopoPyScale import topoclass as tc from matplotlib import pyplot as plt # ========= STEP 1 ========== # Load Configuration config_file = './config.yml' mp = tc . Topoclass ( config_file ) # ======== STEP 2 =========== # Compute parameters of the DEM (slope, aspect, sky view factor) mp . compute_dem_param () mp . extract_topo_param () # ========= STEP 3 ========== # compute solar geometry and horizon angles mp . compute_solar_geometry () mp . compute_horizon () # ========= STEP 4 ========== # Perform the downscaling mp . downscale_climate () # ========= STEP 5 ========== # explore the downscaled dataset. For instance the temperature difference between each point and the first one ( mp . downscaled_pts . t - mp . downscaled_pts . t . isel ( point_id = 0 )) . plot () plt . show () # ========= STEP 6 ========== # Export output to desired format mp . to_netcdf () TopoClass will create a file structure in the project folder (see below). TopoPyScale assumes you have a DEM in GeoTiFF, and a set of climate data in netcdf (following ERA5 variable conventions). TopoPyScale can easier segment the DEM using clustering (e.g. K-mean), or a list of predefined point coordinates in pts_list.csv can be provided. Make sure all parameters in config.yml are correct. my_project/ \u251c\u2500\u2500 inputs/ \u251c\u2500\u2500 dem/ \u251c\u2500\u2500 my_dem.tif \u2514\u2500\u2500 pts_list.csv (optional) \u2514\u2500\u2500 climate/ \u251c\u2500\u2500 PLEV*.nc \u2514\u2500\u2500 SURF*.nc \u251c\u2500\u2500 outputs/ \u251c\u2500\u2500 tmp/ \u2514\u2500\u2500 config.yml","title":"Basic usage"},{"location":"2_quickstart/#plotting","text":"TopoPyScale incudes a number of plotting tools: # To plot cluster map: mp . toposub . plot_clusters_map () # To plot sky view factor or any other variable mp . toposub . plot_clusters_map ( var = 'svf' , cmap = plt . cm . viridis )","title":"Plotting"},{"location":"2_quickstart/#comparison-to-observations","text":"TopoPyScale includes tools to fetch weather station observations from public databases: WMO Norwegian meteoroligical Institue MetNO FROST API These functions are available into topo_obs.py . topo_compare.py includes functions to estimate and also correct bias between downscaled and a reference timeseries.","title":"Comparison to Observations"},{"location":"3_configurationFile/","text":"Project Configuration Project Organisation To run a TopoPyScale project, you need to have the following file structure: my_project/ \u251c\u2500\u2500 inputs/ \u251c\u2500\u2500 dem/ \u251c\u2500\u2500 my_dem.tif \u2514\u2500\u2500 pts_list.csv (optional) \u2514\u2500\u2500 climate/ \u251c\u2500\u2500 PLEV*.nc \u2514\u2500\u2500 SURF*.nc \u251c\u2500\u2500 outputs/ \u251c\u2500\u2500 tmp/ \u2514\u2500\u2500 config.yml File config.yml The configuration file contains all parameters needed to run a downscaling work. It includes general information about the job, as well as specific routine and values. Examples of config.yml file can be found in the repository TopoPyScale_examples . The configuration consists of a YAML file, which is a common standard for storing configurations. Further help on YAML syntax can be found here , and the Python packages pyyaml and Munch allows to interact with such kind of file. For TopoPyScale, the configuration file must contain at least the sections: project : name : Name of the project description : Describe the project authors : - Author 1 (can add contact and affiliation here) - Author 2 - Author 3 date : Date at which the project is ran. This is metadata directory : /path/to/project/ # start and end date of the timeperiod of interest start : 2018-01-01 end : 2018-01-31 # Indicate the number of core to use CPU_cores : 4 # indicate which climate data to use. Currently only era5 available (see climate section below) climate : era5 #..................................................................................................... climate : # For now TopoPyScale only supports ERA5-reanalysis input climate data era5 : path : inputs/climate/ product : reanalysis timestep : 1H # Choose pressure levels relevan to your project and evailable in ERA5 Pressure Levels plevels : [ 700 , 750 , 775 , 800 , 825 , 850 , 875 , 900 , 925 , 950 , 975 , 1000 ] # Number of threads to request downloads with cdsapi download_threads : 12 #..................................................................................................... dem : # Name of the dem file. Must be a raster. file : myDEM.tif # projection EPSG code epsg : 32632 # horizon increment angle in degrees horizon_increments : 10 #..................................................................................................... sampling : # choose downscaling using dem segmentation 'toposub' or a list of points 'points'. Possible values: toposub, points method : toposub # In case method == 'points', indicate a file with a list of points and the point coordinate projection EPSG code points : csv_file : pt_list.csv epsg : 4326 # In case method == 'toposub' toposub : # clustering method available: kmean, minibatchkmean clustering_method : minibatchkmean n_clusters : 50 random_seed : 2 #..................................................................................................... toposcale : # interpolation methods available: linear or idw interpolation_method : idw pt_sampling_method : nearest # Turn ON/OFF terrain contribution to longwave LW_terrain_contribution : True The file config.yml is parsed by TopoPyScale at the time the class topoclass('config.yml') is created. File csv format for a list of points The list of points is a comma-separated value file which must contain at least the fields x,y . All other columns will be loaded into a dataframe and can be used for further analysis (but won't be used by TopoPyScale). An example of a list of points: Name,stn_number,latitude,longitude,x,y Finsevatne,SN25830,60.5938,7.527,419320.867306002,6718447.86246835 Fet-I-Eidfjord,SN49800,60.4085,7.2798,405243.856317655,6698143.36494597 Skurdevik\u00e5i,SN29900,60.3778,7.5693,421114.679132306,6694343.36865902 Midtstova,SN53530,60.6563,7.2755,405730.30171528,6725742.26010349 FV50-Vestredalen,SN53990,60.7418,7.5748,422296.164018722,6734871.61164008 Klevavatnet,SN53480,60.7192,7.2085,402259.379226592,6732844.21093029","title":"Project Configuration"},{"location":"3_configurationFile/#project-configuration","text":"","title":"Project Configuration"},{"location":"3_configurationFile/#project-organisation","text":"To run a TopoPyScale project, you need to have the following file structure: my_project/ \u251c\u2500\u2500 inputs/ \u251c\u2500\u2500 dem/ \u251c\u2500\u2500 my_dem.tif \u2514\u2500\u2500 pts_list.csv (optional) \u2514\u2500\u2500 climate/ \u251c\u2500\u2500 PLEV*.nc \u2514\u2500\u2500 SURF*.nc \u251c\u2500\u2500 outputs/ \u251c\u2500\u2500 tmp/ \u2514\u2500\u2500 config.yml","title":"Project Organisation"},{"location":"3_configurationFile/#file-configyml","text":"The configuration file contains all parameters needed to run a downscaling work. It includes general information about the job, as well as specific routine and values. Examples of config.yml file can be found in the repository TopoPyScale_examples . The configuration consists of a YAML file, which is a common standard for storing configurations. Further help on YAML syntax can be found here , and the Python packages pyyaml and Munch allows to interact with such kind of file. For TopoPyScale, the configuration file must contain at least the sections: project : name : Name of the project description : Describe the project authors : - Author 1 (can add contact and affiliation here) - Author 2 - Author 3 date : Date at which the project is ran. This is metadata directory : /path/to/project/ # start and end date of the timeperiod of interest start : 2018-01-01 end : 2018-01-31 # Indicate the number of core to use CPU_cores : 4 # indicate which climate data to use. Currently only era5 available (see climate section below) climate : era5 #..................................................................................................... climate : # For now TopoPyScale only supports ERA5-reanalysis input climate data era5 : path : inputs/climate/ product : reanalysis timestep : 1H # Choose pressure levels relevan to your project and evailable in ERA5 Pressure Levels plevels : [ 700 , 750 , 775 , 800 , 825 , 850 , 875 , 900 , 925 , 950 , 975 , 1000 ] # Number of threads to request downloads with cdsapi download_threads : 12 #..................................................................................................... dem : # Name of the dem file. Must be a raster. file : myDEM.tif # projection EPSG code epsg : 32632 # horizon increment angle in degrees horizon_increments : 10 #..................................................................................................... sampling : # choose downscaling using dem segmentation 'toposub' or a list of points 'points'. Possible values: toposub, points method : toposub # In case method == 'points', indicate a file with a list of points and the point coordinate projection EPSG code points : csv_file : pt_list.csv epsg : 4326 # In case method == 'toposub' toposub : # clustering method available: kmean, minibatchkmean clustering_method : minibatchkmean n_clusters : 50 random_seed : 2 #..................................................................................................... toposcale : # interpolation methods available: linear or idw interpolation_method : idw pt_sampling_method : nearest # Turn ON/OFF terrain contribution to longwave LW_terrain_contribution : True The file config.yml is parsed by TopoPyScale at the time the class topoclass('config.yml') is created.","title":"File config.yml"},{"location":"3_configurationFile/#file-csv-format-for-a-list-of-points","text":"The list of points is a comma-separated value file which must contain at least the fields x,y . All other columns will be loaded into a dataframe and can be used for further analysis (but won't be used by TopoPyScale). An example of a list of points: Name,stn_number,latitude,longitude,x,y Finsevatne,SN25830,60.5938,7.527,419320.867306002,6718447.86246835 Fet-I-Eidfjord,SN49800,60.4085,7.2798,405243.856317655,6698143.36494597 Skurdevik\u00e5i,SN29900,60.3778,7.5693,421114.679132306,6694343.36865902 Midtstova,SN53530,60.6563,7.2755,405730.30171528,6725742.26010349 FV50-Vestredalen,SN53990,60.7418,7.5748,422296.164018722,6734871.61164008 Klevavatnet,SN53480,60.7192,7.2085,402259.379226592,6732844.21093029","title":"File csv format for a list of points"},{"location":"4_datasetSources/","text":"Dataset Sources TopoPyScale relies on two main types of data: climate and topographical data. These data can be freely available for your region and we list here a number of public and open access data repository relevant to TopoPyScale. Climate data ERA5-copernicus ERA5 comes in two parts: Hourly land Hourly pressure levels Digital Elevation models SRTM (global) SRTM is a global product produced by NASA . There are 2 basic products available, the 30m and 90m resolution. Useful links to download tiles: https://search.earthdata.nasa.gov/search?q=SRTM https://www2.jpl.nasa.gov/srtm/ https://dwtkns.com/srtm30m/ ArcticDEM (global/Arctic) ArcticDEM provides DEMs for the global Arctic region at a fine spatial resolution. Observation Dataset WMO stations (Global) Norwegian meteorological Institute Frost API (Norway) NOAA Hourly (Global)","title":"Datasets"},{"location":"4_datasetSources/#dataset-sources","text":"TopoPyScale relies on two main types of data: climate and topographical data. These data can be freely available for your region and we list here a number of public and open access data repository relevant to TopoPyScale.","title":"Dataset Sources"},{"location":"4_datasetSources/#climate-data","text":"","title":"Climate data"},{"location":"4_datasetSources/#era5-copernicus","text":"ERA5 comes in two parts: Hourly land Hourly pressure levels","title":"ERA5-copernicus"},{"location":"4_datasetSources/#digital-elevation-models","text":"","title":"Digital Elevation models"},{"location":"4_datasetSources/#srtm-global","text":"SRTM is a global product produced by NASA . There are 2 basic products available, the 30m and 90m resolution. Useful links to download tiles: https://search.earthdata.nasa.gov/search?q=SRTM https://www2.jpl.nasa.gov/srtm/ https://dwtkns.com/srtm30m/","title":"SRTM (global)"},{"location":"4_datasetSources/#arcticdem-globalarctic","text":"ArcticDEM provides DEMs for the global Arctic region at a fine spatial resolution.","title":"ArcticDEM (global/Arctic)"},{"location":"4_datasetSources/#observation-dataset","text":"WMO stations (Global) Norwegian meteorological Institute Frost API (Norway) NOAA Hourly (Global)","title":"Observation Dataset"},{"location":"5_models/","text":"Output Formats TopoPyScale include a number of functions to export the downscaled timeseries to be compatible with some established energy and mass balance land-surface models for snow and permafrost physics, or hydrology. All these functions are available into the file topo_export.py and are integrated to the class topoclass . FSM FSM TopoPyScale includes tools to interact and work with FSM. Those are available in the file topo_sim.py . Cryogrid Cryogrid is a model used for simulating the thermal regime of the ground, particularly applied to permafrost and hydrological application. Snowmodel Snowmodel is snow model focused on wind redistribution. CROCUS CROCUS is a 1D physical model developed by Meteo France to simulate the evolution of snowpack state variables and snow metamorphic path. SNOWPACK SNOWPACK is a physical model developped at SLF, Switzerland to simulate the evolution a snowpack state variables and snow metamorphic path. GEOTOP","title":"Outputs"},{"location":"5_models/#output-formats","text":"TopoPyScale include a number of functions to export the downscaled timeseries to be compatible with some established energy and mass balance land-surface models for snow and permafrost physics, or hydrology. All these functions are available into the file topo_export.py and are integrated to the class topoclass .","title":"Output Formats"},{"location":"5_models/#fsm","text":"FSM TopoPyScale includes tools to interact and work with FSM. Those are available in the file topo_sim.py .","title":"FSM"},{"location":"5_models/#cryogrid","text":"Cryogrid is a model used for simulating the thermal regime of the ground, particularly applied to permafrost and hydrological application.","title":"Cryogrid"},{"location":"5_models/#snowmodel","text":"Snowmodel is snow model focused on wind redistribution.","title":"Snowmodel"},{"location":"5_models/#crocus","text":"CROCUS is a 1D physical model developed by Meteo France to simulate the evolution of snowpack state variables and snow metamorphic path.","title":"CROCUS"},{"location":"5_models/#snowpack","text":"SNOWPACK is a physical model developped at SLF, Switzerland to simulate the evolution a snowpack state variables and snow metamorphic path.","title":"SNOWPACK"},{"location":"5_models/#geotop","text":"","title":"GEOTOP"},{"location":"TopoPyScale.fetch_dem/","text":"module TopoPyScale.fetch_dem Methods to fetch DEM from various public repository TODO: [ ] SRTM [ ] ArcticDEM [ ] ASTER dem [ ] Norwegian DEM function fetch_dem fetch_dem ( dem_dir , extent , dem_epsg , dem_file ) Function to fetch DEM data from SRTM and potentially other sources Args: dem_dir (str): path to dem folder extent (list): list of spatial extent in lat-lon [latN, latS, lonW, lonE] epsg (int): epsg projection code dem_file (str): filename of the downloaded DEM. must be myfile.tif This file was automatically generated via lazydocs .","title":"fetch_dem"},{"location":"TopoPyScale.fetch_dem/#module-topopyscalefetch_dem","text":"Methods to fetch DEM from various public repository TODO: [ ] SRTM [ ] ArcticDEM [ ] ASTER dem [ ] Norwegian DEM","title":"module TopoPyScale.fetch_dem"},{"location":"TopoPyScale.fetch_dem/#function-fetch_dem","text":"fetch_dem ( dem_dir , extent , dem_epsg , dem_file ) Function to fetch DEM data from SRTM and potentially other sources Args: dem_dir (str): path to dem folder extent (list): list of spatial extent in lat-lon [latN, latS, lonW, lonE] epsg (int): epsg projection code dem_file (str): filename of the downloaded DEM. must be myfile.tif This file was automatically generated via lazydocs .","title":"function fetch_dem"},{"location":"TopoPyScale.fetch_era5/","text":"module TopoPyScale.fetch_era5 Retrieve ecmwf data with cdsapi. J. Fiddes, Origin implementation S. Filhol adapted in 2021 function retrieve_era5 retrieve_era5 ( product , startDate , endDate , eraDir , latN , latS , lonE , lonW , step , num_threads = 10 , surf_plev = 'surf' , plevels = None ) Sets up era5 surface retrieval. * Creates list of year/month pairs to iterate through. * MARS retrievals are most efficient when subset by time. * Identifies preexisting downloads if restarted. * Calls api using parallel function. Args: product : \"reanalysis\" (HRES) or \"ensemble_members\" (EDA) startDate: endDate: eraDir : directory to write output latN : north latitude of bbox latS : south latitude of bbox lonE : easterly lon of bbox lonW : westerly lon of bbox step : timestep to use: 1, 3, 6 num_threads : number of threads to use for downloading data surf_plev : download surface single level or pressure level product: 'surf' or 'plev' Returns: Monthly era surface files stored in disk. function era5_request_surf era5_request_surf ( dataset , year , month , bbox , target , product , time ) CDS surface api call Args: dataset (str): copernicus dataset (era5) year (str or list): year of interest month (str or list): month of interest bbox (list): bonding box in lat-lon target (str): filename product (str): type of model run. defaul: reanalysis time (str or list): hours for which to download data Returns: Store to disk dataset as indicated function era5_request_plev era5_request_plev ( dataset , year , month , bbox , target , product , time , plevels ) CDS plevel api call Args: dataset (str): copernicus dataset (era5) year (str or list): year of interest month (str or list): month of interest bbox (list): bonding box in lat-lon target (str): filename product (str): type of model run. defaul: reanalysis time (str or list): hours to query plevels (str or list): pressure levels to query Returns: Store to disk dataset as indicated This file was automatically generated via lazydocs .","title":"fetch_era5"},{"location":"TopoPyScale.fetch_era5/#module-topopyscalefetch_era5","text":"Retrieve ecmwf data with cdsapi. J. Fiddes, Origin implementation S. Filhol adapted in 2021","title":"module TopoPyScale.fetch_era5"},{"location":"TopoPyScale.fetch_era5/#function-retrieve_era5","text":"retrieve_era5 ( product , startDate , endDate , eraDir , latN , latS , lonE , lonW , step , num_threads = 10 , surf_plev = 'surf' , plevels = None ) Sets up era5 surface retrieval. * Creates list of year/month pairs to iterate through. * MARS retrievals are most efficient when subset by time. * Identifies preexisting downloads if restarted. * Calls api using parallel function. Args: product : \"reanalysis\" (HRES) or \"ensemble_members\" (EDA) startDate: endDate: eraDir : directory to write output latN : north latitude of bbox latS : south latitude of bbox lonE : easterly lon of bbox lonW : westerly lon of bbox step : timestep to use: 1, 3, 6 num_threads : number of threads to use for downloading data surf_plev : download surface single level or pressure level product: 'surf' or 'plev' Returns: Monthly era surface files stored in disk.","title":"function retrieve_era5"},{"location":"TopoPyScale.fetch_era5/#function-era5_request_surf","text":"era5_request_surf ( dataset , year , month , bbox , target , product , time ) CDS surface api call Args: dataset (str): copernicus dataset (era5) year (str or list): year of interest month (str or list): month of interest bbox (list): bonding box in lat-lon target (str): filename product (str): type of model run. defaul: reanalysis time (str or list): hours for which to download data Returns: Store to disk dataset as indicated","title":"function era5_request_surf"},{"location":"TopoPyScale.fetch_era5/#function-era5_request_plev","text":"era5_request_plev ( dataset , year , month , bbox , target , product , time , plevels ) CDS plevel api call Args: dataset (str): copernicus dataset (era5) year (str or list): year of interest month (str or list): month of interest bbox (list): bonding box in lat-lon target (str): filename product (str): type of model run. defaul: reanalysis time (str or list): hours to query plevels (str or list): pressure levels to query Returns: Store to disk dataset as indicated This file was automatically generated via lazydocs .","title":"function era5_request_plev"},{"location":"TopoPyScale/","text":"module TopoPyScale This file was automatically generated via lazydocs .","title":"TopoPyScale"},{"location":"TopoPyScale/#module-topopyscale","text":"This file was automatically generated via lazydocs .","title":"module TopoPyScale"},{"location":"TopoPyScale.meteo_util/","text":"module TopoPyScale.meteo_util Global Variables var_era_plevel var_era_surf g R eps0 S0 function partition_snow partition_snow ( precip , temp , rh = None , sp = None , method = 'continuous' , tair_low_thresh = 272.15 , tair_high_thresh = 274.15 ) Function to partition precipitation in between rain vs snow based on temperature threshold and mixing around freezing. The methods to partition snow/rain precipitation are: - continuous: method implemented into Cryogrid. - jennings2018 methods are from the publication Jennings et al (2018). DOI: https://doi.org/10.1038/s41467-018-03629-7 Args: precip (array): 1D array, precipitation in mm/hr temp (arrray): 1D array, air temperature in K rh (array): 1D array, relative humidity in % sp (array): 1D array, surface pressure in Pa method (str): 'continuous', 'Jennings2018_bivariate', 'Jennings2018_trivariate'. tair_low_thresh (float): lower temperature threshold under which all precip is snow. degree K tair_high_thresh (float): higher temperature threshold under which all precip is rain. degree K Returns: array : 1D array rain array : 1D array snow function q_2_rh q_2_rh ( temp , pressure , qair ) Function to convert specific humidity (q) to relative humidity (RH) following Bolton (1980) Args: temp (array): temperature in degree K pressure (array): aire pressure in Pa qair (array): specific humidity in kg/kg Returns: array : relative humidity in the range of [0-1] function mixing_ratio mixing_ratio ( ds , var ) Function to compute mixing ratio Args: ds : dataset (following ERA5 variable convention) q-specific humidity in kg/kg var : dictionnary of variable name (for ERA surf / ERA plevel) Returns: dataset with new variable 'w' function t_rh_2_dewT t_rh_2_dewT ( ds , var ) Function to compute dea temperature from air temperature and relative humidity Args: ds : dataset with temperature ('t') and relative humidity ('r') variables var : dictionnary of variable name (for ERA surf / ERA plevel) Returns: dataset : with added variable function dewT_2_q_magnus dewT_2_q_magnus ( ds , var ) A version of the Magnus formula with the AERK parameters. AERK from Alduchov and Eskridge (1996) Note, e=Magnus(tdc) and es=Magnus(tc) Args: ds : dataset with var : dictionnary of variable name (for ERA surf / ERA plevel) Returns: array : specific humidity in kg/kg function vapor_pressure vapor_pressure ( ds , var ) Function to compute Vapor pressure from mixing ratio based on 2nd order Taylor series expansion Args: ds (dataset): dataset with pressure and mix_ratio variables var (dict): variable name correspondance Returns: dataset : input dataset with new vp columns This file was automatically generated via lazydocs .","title":"meteo_util"},{"location":"TopoPyScale.meteo_util/#module-topopyscalemeteo_util","text":"","title":"module TopoPyScale.meteo_util"},{"location":"TopoPyScale.meteo_util/#global-variables","text":"var_era_plevel var_era_surf g R eps0 S0","title":"Global Variables"},{"location":"TopoPyScale.meteo_util/#function-partition_snow","text":"partition_snow ( precip , temp , rh = None , sp = None , method = 'continuous' , tair_low_thresh = 272.15 , tair_high_thresh = 274.15 ) Function to partition precipitation in between rain vs snow based on temperature threshold and mixing around freezing. The methods to partition snow/rain precipitation are: - continuous: method implemented into Cryogrid. - jennings2018 methods are from the publication Jennings et al (2018). DOI: https://doi.org/10.1038/s41467-018-03629-7 Args: precip (array): 1D array, precipitation in mm/hr temp (arrray): 1D array, air temperature in K rh (array): 1D array, relative humidity in % sp (array): 1D array, surface pressure in Pa method (str): 'continuous', 'Jennings2018_bivariate', 'Jennings2018_trivariate'. tair_low_thresh (float): lower temperature threshold under which all precip is snow. degree K tair_high_thresh (float): higher temperature threshold under which all precip is rain. degree K Returns: array : 1D array rain array : 1D array snow","title":"function partition_snow"},{"location":"TopoPyScale.meteo_util/#function-q_2_rh","text":"q_2_rh ( temp , pressure , qair ) Function to convert specific humidity (q) to relative humidity (RH) following Bolton (1980) Args: temp (array): temperature in degree K pressure (array): aire pressure in Pa qair (array): specific humidity in kg/kg Returns: array : relative humidity in the range of [0-1]","title":"function q_2_rh"},{"location":"TopoPyScale.meteo_util/#function-mixing_ratio","text":"mixing_ratio ( ds , var ) Function to compute mixing ratio Args: ds : dataset (following ERA5 variable convention) q-specific humidity in kg/kg var : dictionnary of variable name (for ERA surf / ERA plevel) Returns: dataset with new variable 'w'","title":"function mixing_ratio"},{"location":"TopoPyScale.meteo_util/#function-t_rh_2_dewt","text":"t_rh_2_dewT ( ds , var ) Function to compute dea temperature from air temperature and relative humidity Args: ds : dataset with temperature ('t') and relative humidity ('r') variables var : dictionnary of variable name (for ERA surf / ERA plevel) Returns: dataset : with added variable","title":"function t_rh_2_dewT"},{"location":"TopoPyScale.meteo_util/#function-dewt_2_q_magnus","text":"dewT_2_q_magnus ( ds , var ) A version of the Magnus formula with the AERK parameters. AERK from Alduchov and Eskridge (1996) Note, e=Magnus(tdc) and es=Magnus(tc) Args: ds : dataset with var : dictionnary of variable name (for ERA surf / ERA plevel) Returns: array : specific humidity in kg/kg","title":"function dewT_2_q_magnus"},{"location":"TopoPyScale.meteo_util/#function-vapor_pressure","text":"vapor_pressure ( ds , var ) Function to compute Vapor pressure from mixing ratio based on 2nd order Taylor series expansion Args: ds (dataset): dataset with pressure and mix_ratio variables var (dict): variable name correspondance Returns: dataset : input dataset with new vp columns This file was automatically generated via lazydocs .","title":"function vapor_pressure"},{"location":"TopoPyScale.precip_orographic/","text":"module TopoPyScale.precip_orographic Implementation of orographic correction for precipitation field following the XX method. S. Filhol, December 2021 WARNING: in development, NOT READY References: - https://journals.ametsoc.org/view/journals/atsc/61/12/1520-0469_2004_061_1377_altoop_2.0.co_2.xml - TODO: read Smith and Barstad 2004, https://journals.ametsoc.org/view/journals/atsc/61/12/1520-0469_2004_061_1377_altoop_2.0.co_2.xml read Thomas paper as well as Aurora talk to Andy beaucse of: https://github.com/pism/LinearTheoryOrographicPrecipitation Thomas' Matlab implementation from ERA5: https://github.com/TVSchuler/Sval_Imp_matlab/blob/0088e31877334428cddcb4ff9b87bcb3973c4bbb/precipitation/LT_matlab_2016_09_16.m#L126 For validation, we could check against ERA5 Land, or CARRA in Svalbard This file was automatically generated via lazydocs .","title":"TopoPyScale.precip orographic"},{"location":"TopoPyScale.precip_orographic/#module-topopyscaleprecip_orographic","text":"Implementation of orographic correction for precipitation field following the XX method. S. Filhol, December 2021 WARNING: in development, NOT READY References: - https://journals.ametsoc.org/view/journals/atsc/61/12/1520-0469_2004_061_1377_altoop_2.0.co_2.xml - TODO: read Smith and Barstad 2004, https://journals.ametsoc.org/view/journals/atsc/61/12/1520-0469_2004_061_1377_altoop_2.0.co_2.xml read Thomas paper as well as Aurora talk to Andy beaucse of: https://github.com/pism/LinearTheoryOrographicPrecipitation Thomas' Matlab implementation from ERA5: https://github.com/TVSchuler/Sval_Imp_matlab/blob/0088e31877334428cddcb4ff9b87bcb3973c4bbb/precipitation/LT_matlab_2016_09_16.m#L126 For validation, we could check against ERA5 Land, or CARRA in Svalbard This file was automatically generated via lazydocs .","title":"module TopoPyScale.precip_orographic"},{"location":"TopoPyScale.solar_geom/","text":"module TopoPyScale.solar_geom Function to compute Solar angles S. Filhol, Oct 2021 function get_solar_geom get_solar_geom ( df_position , start_date , end_date , tstep , sr_epsg = '4326' , num_threads = None ) Function to compute solar position for each location given in the dataframe azimuth is define with 0 towards South, negative in W-dir, and posiive towards E-dir Args: df_position (dataframe): point_id as index, latitude, longitude start_date (str): start date \"2014-05-10\" end_date (str : end date \"2015-05-10\" tstep (str): time step, ex: '6H' sr_epsg (str): source EPSG code for the input coordinate num_threads (int): number of threads to parallelize computation on. default is number of core -2 Returns: dataset : solar angles in degrees This file was automatically generated via lazydocs .","title":"solar_geom"},{"location":"TopoPyScale.solar_geom/#module-topopyscalesolar_geom","text":"Function to compute Solar angles S. Filhol, Oct 2021","title":"module TopoPyScale.solar_geom"},{"location":"TopoPyScale.solar_geom/#function-get_solar_geom","text":"get_solar_geom ( df_position , start_date , end_date , tstep , sr_epsg = '4326' , num_threads = None ) Function to compute solar position for each location given in the dataframe azimuth is define with 0 towards South, negative in W-dir, and posiive towards E-dir Args: df_position (dataframe): point_id as index, latitude, longitude start_date (str): start date \"2014-05-10\" end_date (str : end date \"2015-05-10\" tstep (str): time step, ex: '6H' sr_epsg (str): source EPSG code for the input coordinate num_threads (int): number of threads to parallelize computation on. default is number of core -2 Returns: dataset : solar angles in degrees This file was automatically generated via lazydocs .","title":"function get_solar_geom"},{"location":"TopoPyScale.topo_compare/","text":"module TopoPyScale.topo_compare function correct_trend correct_trend ( df , reference_col = 'obs' , target_col = 'dow' , apply_correction = True ) Function to estimate linear trend correction. Args: df (dataframe): - reference_col (str): name of the reference column, i.e. observation timeseries - target_col (str): name of the target column, i.e. downscale timeseries - apply_correction (bool): applying correction to target data Returns: dict : metrics of the linear trend estimate dataframe (optional): corrected values function correct_seasonal correct_seasonal ( df , reference_col = 'obs' , target_col = 'dow' , plot = True , apply_correction = True ) Function to correct for seasonal signal. 1. it groups all days by day_of_year and extract the median difference. 2. it computes the 31 day rolling mean (padded on both sides) Args: df (dataframe): index must be a datetime, with daily timestep. other columns are reference and target. reference_col (str): name of the reference column, i.e. observation timeseries target_col (str): name of the target column, i.e. downscale timeseries plot (bool): apply_correction (bool): apply correction and return corrected data Returns: dataframe - correction for each day of year, starting on January 1. function obs_vs_downscaled obs_vs_downscaled ( df , reference_col = 'obs' , target_col = 'dow' , trend_correction = True , seasonal_correction = True , param = { 'xlab' : 'Downscaled [unit]' , 'ylab' : 'Observation [unit]' , 'xlim' : ( - 20 , 20 ), 'ylim' : ( - 20 , 20 ), 'title' : None }, plot = 'heatmap' ) Function to compare Observation to Downscaled for one given variable. Args: df (dataframe): pandas dataframe containing corresponding Observation and Downscaled values reference_col (str): name of the reference column. Observation target_col (str): name of the target column. Downscaled timeseries trend_correction (bool): remove trend by applying a linear regression seasonal_correction (bool): remove seasonal signal by deriving median per day of the year and computing the rolling mean over 31d on the median param (dict): parameter for comparison and plotting plot (str): plot type: heatmap or timeseries Returns: dict : metrics of regression and comparison dataframe : dataframe containing the seasonal corrections to applied dataframe : corrected values Inspired by this study: https://reader.elsevier.com/reader/sd/pii/S0048969722015522?token=106483481240DE6206D83D9C7EC9D4990C2C3DE6F669EDE39DCB54FF7495A31CC57BDCF3370A6CA39D09BE293EACDDBB&originRegion=eu-west-1&originCreation=20220316094240 This file was automatically generated via lazydocs .","title":"topo_compare"},{"location":"TopoPyScale.topo_compare/#module-topopyscaletopo_compare","text":"","title":"module TopoPyScale.topo_compare"},{"location":"TopoPyScale.topo_compare/#function-correct_trend","text":"correct_trend ( df , reference_col = 'obs' , target_col = 'dow' , apply_correction = True ) Function to estimate linear trend correction. Args: df (dataframe): - reference_col (str): name of the reference column, i.e. observation timeseries - target_col (str): name of the target column, i.e. downscale timeseries - apply_correction (bool): applying correction to target data Returns: dict : metrics of the linear trend estimate dataframe (optional): corrected values","title":"function correct_trend"},{"location":"TopoPyScale.topo_compare/#function-correct_seasonal","text":"correct_seasonal ( df , reference_col = 'obs' , target_col = 'dow' , plot = True , apply_correction = True ) Function to correct for seasonal signal. 1. it groups all days by day_of_year and extract the median difference. 2. it computes the 31 day rolling mean (padded on both sides) Args: df (dataframe): index must be a datetime, with daily timestep. other columns are reference and target. reference_col (str): name of the reference column, i.e. observation timeseries target_col (str): name of the target column, i.e. downscale timeseries plot (bool): apply_correction (bool): apply correction and return corrected data Returns: dataframe - correction for each day of year, starting on January 1.","title":"function correct_seasonal"},{"location":"TopoPyScale.topo_compare/#function-obs_vs_downscaled","text":"obs_vs_downscaled ( df , reference_col = 'obs' , target_col = 'dow' , trend_correction = True , seasonal_correction = True , param = { 'xlab' : 'Downscaled [unit]' , 'ylab' : 'Observation [unit]' , 'xlim' : ( - 20 , 20 ), 'ylim' : ( - 20 , 20 ), 'title' : None }, plot = 'heatmap' ) Function to compare Observation to Downscaled for one given variable. Args: df (dataframe): pandas dataframe containing corresponding Observation and Downscaled values reference_col (str): name of the reference column. Observation target_col (str): name of the target column. Downscaled timeseries trend_correction (bool): remove trend by applying a linear regression seasonal_correction (bool): remove seasonal signal by deriving median per day of the year and computing the rolling mean over 31d on the median param (dict): parameter for comparison and plotting plot (str): plot type: heatmap or timeseries Returns: dict : metrics of regression and comparison dataframe : dataframe containing the seasonal corrections to applied dataframe : corrected values Inspired by this study: https://reader.elsevier.com/reader/sd/pii/S0048969722015522?token=106483481240DE6206D83D9C7EC9D4990C2C3DE6F669EDE39DCB54FF7495A31CC57BDCF3370A6CA39D09BE293EACDDBB&originRegion=eu-west-1&originCreation=20220316094240 This file was automatically generated via lazydocs .","title":"function obs_vs_downscaled"},{"location":"TopoPyScale.topo_export/","text":"module TopoPyScale.topo_export Functions to export topo_scale output to formats compatible with existing models (e.g. CROCUS, Cryogrid, Snowmodel, ...) S. Filhol, December 2021 TODO; - SPHY forcing (grids) function compute_scaling_and_offset compute_scaling_and_offset ( da , n = 16 ) Compute offset and scale factor for int conversion Args: da (dataarray): of a given variable n (int): number of digits to account for function to_musa to_musa ( ds , df_pts , da_label , fname_met = 'musa_met.nc' , fname_labels = 'musa_labels.nc' , path = 'outputs/' , climate_dataset_name = 'ERA5' , project_authors = 'S. Filhol' ) Function to export to MuSa standard. Args: ds: df_pts: da_labels: fname: path: Returns: Save downscaled timeseries and toposub cluster mapping function to_cryogrid to_cryogrid ( ds , df_pts , fname_format = 'Cryogrid_pt_*.nc' , path = 'outputs/' , label_map = False , da_label = None , snow_partition_method = 'continuous' , climate_dataset_name = 'ERA5' , project_author = 'S. Filhol' ) Function to export TopoPyScale downscaled dataset in a netcdf format compatible for Cryogrid-community model. Args: ds (dataset): downscaled values from topo_scale df_pts (dataframe): with metadata of all point of downscaling fname_format (str): file name for output path (str): path where to export files label_map (bool): export cluster_label map da_label (dataarray): (optional) dataarray containing label value for each pixel of the DEM function to_fsm to_fsm ( ds , fname_format = 'FSM_pt_*.tx' , snow_partition_method = 'continuous' ) Function to export data for FSM. Args: ds (dataset): downscaled_pts, df_pts (dataframe): toposub.df_centroids, fname_format (str pattern): output format of filename snow_partition_method (str): snow/rain partitioning method: default 'jennings2018_trivariate' format is a text file with the following columns year month day hour SW LW Sf Rf Ta RH Ua Ps (yyyy) (mm) (dd) (hh) (W/m2) (W/m2) (kg/m2/s) (kg/m2/s) (K) (RH 0-100) (m/s) (Pa) See README.md file from FSM source code for further details TODO: Check unit DONE jf Check format is compatible with compiled model DONE jf ensure ds.point_id.values always function to_micromet_single_station to_micromet_single_station ( ds , df_pts , fname_format = 'Snowmodel_pt_*.csv' , na_values =- 9999 , headers = False ) Function to export TopoScale output in the format for Listn's Snowmodel (using Micromet). One CSV file per point_id Args: ds (dataset): TopoPyScale xarray dataset, downscaled product df_pts (dataframe): with point list info (x,y,elevation,slope,aspect,svf,...) fname_format (str): filename format. point_id is inserted where * is na_values (int): na_value default headers (bool): add headers to file Example of the compatible format for micromet (headers should be removed to run the model: year mo dy hr stn_id easting northing elevation Tair RH speed dir precip (yyyy) (mm) (dd) (hh.hh) (number) (m) (m) (m) (C) (%) (m/s) (deg) (mm/dt) 2002 10 1 12.00 101 426340.0 4411238.0 3598.0 0.92 57.77 4.80 238.29 -9999.00 2002 10 2 12.00 101 426340.0 4411238.0 3598.0 -3.02 78.77 -9999.00 -9999.00 -9999.00 2002 10 8 12.00 101 426340.0 4411238.0 3598.0 -5.02 88.77 -9999.00 -9999.00 -9999.00 function to_crocus to_crocus ( ds , df_pts , fname_format = 'CROCUS_pt_*.nc' , scale_precip = 1 , climate_dataset_name = 'ERA5' , project_author = 'S. Filhol' , snow_partition_method = 'continuous' ) Functiont to export toposcale output to CROCUS netcdf format. Generates one file per point_id Args: ds (dataset): Toposcale downscaled dataset. df_pts (dataframe): with point list info (x,y,elevation,slope,aspect,svf,...) fname_format (str): filename format. point_id is inserted where * is scale_precip (float): scaling factor to apply on precipitation. Default is 1 climate_dataset_name (str): name of original climate dataset. Default 'ERA5', project_author (str): name of project author(s) snow_partition_method (str): snow/rain partitioning method: default 'jennings2018_trivariate' function to_snowpack to_snowpack ( ds , fname_format = 'smet_pt_*.tx' ) Function to export data for snowpack model as smet. https://models.slf.ch/docserver/meteoio/SMET_specifications.pdf Args: ds (dataset): downscaled_pts, df_pts (dataframe): toposub.df_centroids, fname_format (str pattern): output format of filename format is a text file with the following columns (coulumns can vary) SMET 1.1 ASCII [HEADER] station_id = meteoc1 station_name = WFJ2 latitude = 46.829650 longitude = 9.809328 altitude = 2539.0 easting = 780851.861845 northing = 189232.420554 epsg = 21781 nodata = -999 tz = 0 plot_unit = time K - m/s \u00b0 W/m2 W/m2 kg/m2 - plot_description = time air_temperature relative_humidity wind_velocity wind_direction incoming_short_wave_radiation incoming_long_wave_radiation water_equivalent_precipitation_sum - plot_color = 0x000000 0x8324A4 0x50CBDB 0x297E24 0x64DD78 0xF9CA25 0xD99521 0x2431A4 0xA0A0A0 plot_min = -999 253.15 0 0 0 0 150 0 -999 plot_max = -999 283.15 1 30 360 1400 400 20 -999 fields = timestamp TA RH VW DW ISWR ILWR PSUM PINT [DATA] 2014-09-01T00:00:00 271.44 1.000 5.9 342 67 304 0.000 0.495 2014-09-01T01:00:00 271.50 0.973 6.7 343 128 300 0.166 0.663 2014-09-01T02:00:00 271.46 0.968 7.4 343 244 286 0.197 0.788 2014-09-01T03:00:00 271.41 0.975 7.6 345 432 273 0.156 0.626 2014-09-01T04:00:00 271.57 0.959 7.8 347 639 249 0.115 0.462 2014-09-01T05:00:00 271.52 0.965 8.2 350 632 261 0.081 0.323 function to_geotop to_geotop ( ds , fname_format = 'geotop_pt_*.txt' ) Function to export data for snowpack model as smet. https://models.slf.ch/docserver/meteoio/SMET_specifications.pdf Args: ds (dataset): downscaled_pts, df_pts (dataframe): toposub.df_centroids, fname_format (str pattern): output format of filename Date format = DD/MM/YYYY hh:mm Air temperature = Degree Celsius Relative Humidity = % Radiations components such SWin, SWout, LWin = W/m2 Precipitation = mm/hr Wind speed = m/s Wind direction = degree Air pressure = mbar The format of the file is in *.txt format. format is a text file with the following columns (coulumns can vary) Date,AirT,WindS,WindDr,RelHum,Swglob,Swout,Lwin,Lwout,Iprec,AirPressure 01/09/2015 00:00,1.5,0.7,165,59,0,0,223.9,315.5,0,581.02897 01/09/2015 01:00,1.2,0.6,270,59,0,0,261.8,319,0,581.02897 01/09/2015 02:00,0.8,0.5,152,61,0,0,229.9,312.2,0,581.02897 01/09/2015 03:00,0.8,0.5,270,60,0,0,226.1,310.8,0,581.02897 01/09/2015 04:00,0.3,0.8,68,60,0,0,215.3,309.6,0,581.02897 01/09/2015 05:00,0.2,0.6,270,62,0,0,230.2,309.1,0,581.02897 01/09/2015 06:00,0.3,0.6,35,62,0,0,222.8,306.7,0,581.02897 01/09/2015 07:00,-0.2,0.3,270,65,0,0,210,305.5,0,581.02897 01/09/2015 08:00,0,0.6,52,59,114,23,218.3,312.3,0,581.02897 01/09/2015 09:00,1.9,1.5,176,57,173,35,220.2,322.8,0,581.02897 01/09/2015 10:00,3.4,1.9,183,47,331,67,245.8,372.6,0,581.02897 This file was automatically generated via lazydocs .","title":"topo_export"},{"location":"TopoPyScale.topo_export/#module-topopyscaletopo_export","text":"Functions to export topo_scale output to formats compatible with existing models (e.g. CROCUS, Cryogrid, Snowmodel, ...) S. Filhol, December 2021 TODO; - SPHY forcing (grids)","title":"module TopoPyScale.topo_export"},{"location":"TopoPyScale.topo_export/#function-compute_scaling_and_offset","text":"compute_scaling_and_offset ( da , n = 16 ) Compute offset and scale factor for int conversion Args: da (dataarray): of a given variable n (int): number of digits to account for","title":"function compute_scaling_and_offset"},{"location":"TopoPyScale.topo_export/#function-to_musa","text":"to_musa ( ds , df_pts , da_label , fname_met = 'musa_met.nc' , fname_labels = 'musa_labels.nc' , path = 'outputs/' , climate_dataset_name = 'ERA5' , project_authors = 'S. Filhol' ) Function to export to MuSa standard. Args: ds: df_pts: da_labels: fname: path: Returns: Save downscaled timeseries and toposub cluster mapping","title":"function to_musa"},{"location":"TopoPyScale.topo_export/#function-to_cryogrid","text":"to_cryogrid ( ds , df_pts , fname_format = 'Cryogrid_pt_*.nc' , path = 'outputs/' , label_map = False , da_label = None , snow_partition_method = 'continuous' , climate_dataset_name = 'ERA5' , project_author = 'S. Filhol' ) Function to export TopoPyScale downscaled dataset in a netcdf format compatible for Cryogrid-community model. Args: ds (dataset): downscaled values from topo_scale df_pts (dataframe): with metadata of all point of downscaling fname_format (str): file name for output path (str): path where to export files label_map (bool): export cluster_label map da_label (dataarray): (optional) dataarray containing label value for each pixel of the DEM","title":"function to_cryogrid"},{"location":"TopoPyScale.topo_export/#function-to_fsm","text":"to_fsm ( ds , fname_format = 'FSM_pt_*.tx' , snow_partition_method = 'continuous' ) Function to export data for FSM. Args: ds (dataset): downscaled_pts, df_pts (dataframe): toposub.df_centroids, fname_format (str pattern): output format of filename snow_partition_method (str): snow/rain partitioning method: default 'jennings2018_trivariate' format is a text file with the following columns year month day hour SW LW Sf Rf Ta RH Ua Ps (yyyy) (mm) (dd) (hh) (W/m2) (W/m2) (kg/m2/s) (kg/m2/s) (K) (RH 0-100) (m/s) (Pa) See README.md file from FSM source code for further details TODO: Check unit DONE jf Check format is compatible with compiled model DONE jf ensure ds.point_id.values always","title":"function to_fsm"},{"location":"TopoPyScale.topo_export/#function-to_micromet_single_station","text":"to_micromet_single_station ( ds , df_pts , fname_format = 'Snowmodel_pt_*.csv' , na_values =- 9999 , headers = False ) Function to export TopoScale output in the format for Listn's Snowmodel (using Micromet). One CSV file per point_id Args: ds (dataset): TopoPyScale xarray dataset, downscaled product df_pts (dataframe): with point list info (x,y,elevation,slope,aspect,svf,...) fname_format (str): filename format. point_id is inserted where * is na_values (int): na_value default headers (bool): add headers to file Example of the compatible format for micromet (headers should be removed to run the model: year mo dy hr stn_id easting northing elevation Tair RH speed dir precip (yyyy) (mm) (dd) (hh.hh) (number) (m) (m) (m) (C) (%) (m/s) (deg) (mm/dt) 2002 10 1 12.00 101 426340.0 4411238.0 3598.0 0.92 57.77 4.80 238.29 -9999.00 2002 10 2 12.00 101 426340.0 4411238.0 3598.0 -3.02 78.77 -9999.00 -9999.00 -9999.00 2002 10 8 12.00 101 426340.0 4411238.0 3598.0 -5.02 88.77 -9999.00 -9999.00 -9999.00","title":"function to_micromet_single_station"},{"location":"TopoPyScale.topo_export/#function-to_crocus","text":"to_crocus ( ds , df_pts , fname_format = 'CROCUS_pt_*.nc' , scale_precip = 1 , climate_dataset_name = 'ERA5' , project_author = 'S. Filhol' , snow_partition_method = 'continuous' ) Functiont to export toposcale output to CROCUS netcdf format. Generates one file per point_id Args: ds (dataset): Toposcale downscaled dataset. df_pts (dataframe): with point list info (x,y,elevation,slope,aspect,svf,...) fname_format (str): filename format. point_id is inserted where * is scale_precip (float): scaling factor to apply on precipitation. Default is 1 climate_dataset_name (str): name of original climate dataset. Default 'ERA5', project_author (str): name of project author(s) snow_partition_method (str): snow/rain partitioning method: default 'jennings2018_trivariate'","title":"function to_crocus"},{"location":"TopoPyScale.topo_export/#function-to_snowpack","text":"to_snowpack ( ds , fname_format = 'smet_pt_*.tx' ) Function to export data for snowpack model as smet. https://models.slf.ch/docserver/meteoio/SMET_specifications.pdf Args: ds (dataset): downscaled_pts, df_pts (dataframe): toposub.df_centroids, fname_format (str pattern): output format of filename format is a text file with the following columns (coulumns can vary) SMET 1.1 ASCII [HEADER] station_id = meteoc1 station_name = WFJ2 latitude = 46.829650 longitude = 9.809328 altitude = 2539.0 easting = 780851.861845 northing = 189232.420554 epsg = 21781 nodata = -999 tz = 0 plot_unit = time K - m/s \u00b0 W/m2 W/m2 kg/m2 - plot_description = time air_temperature relative_humidity wind_velocity wind_direction incoming_short_wave_radiation incoming_long_wave_radiation water_equivalent_precipitation_sum - plot_color = 0x000000 0x8324A4 0x50CBDB 0x297E24 0x64DD78 0xF9CA25 0xD99521 0x2431A4 0xA0A0A0 plot_min = -999 253.15 0 0 0 0 150 0 -999 plot_max = -999 283.15 1 30 360 1400 400 20 -999 fields = timestamp TA RH VW DW ISWR ILWR PSUM PINT [DATA] 2014-09-01T00:00:00 271.44 1.000 5.9 342 67 304 0.000 0.495 2014-09-01T01:00:00 271.50 0.973 6.7 343 128 300 0.166 0.663 2014-09-01T02:00:00 271.46 0.968 7.4 343 244 286 0.197 0.788 2014-09-01T03:00:00 271.41 0.975 7.6 345 432 273 0.156 0.626 2014-09-01T04:00:00 271.57 0.959 7.8 347 639 249 0.115 0.462 2014-09-01T05:00:00 271.52 0.965 8.2 350 632 261 0.081 0.323","title":"function to_snowpack"},{"location":"TopoPyScale.topo_export/#function-to_geotop","text":"to_geotop ( ds , fname_format = 'geotop_pt_*.txt' ) Function to export data for snowpack model as smet. https://models.slf.ch/docserver/meteoio/SMET_specifications.pdf Args: ds (dataset): downscaled_pts, df_pts (dataframe): toposub.df_centroids, fname_format (str pattern): output format of filename Date format = DD/MM/YYYY hh:mm Air temperature = Degree Celsius Relative Humidity = % Radiations components such SWin, SWout, LWin = W/m2 Precipitation = mm/hr Wind speed = m/s Wind direction = degree Air pressure = mbar The format of the file is in *.txt format. format is a text file with the following columns (coulumns can vary) Date,AirT,WindS,WindDr,RelHum,Swglob,Swout,Lwin,Lwout,Iprec,AirPressure 01/09/2015 00:00,1.5,0.7,165,59,0,0,223.9,315.5,0,581.02897 01/09/2015 01:00,1.2,0.6,270,59,0,0,261.8,319,0,581.02897 01/09/2015 02:00,0.8,0.5,152,61,0,0,229.9,312.2,0,581.02897 01/09/2015 03:00,0.8,0.5,270,60,0,0,226.1,310.8,0,581.02897 01/09/2015 04:00,0.3,0.8,68,60,0,0,215.3,309.6,0,581.02897 01/09/2015 05:00,0.2,0.6,270,62,0,0,230.2,309.1,0,581.02897 01/09/2015 06:00,0.3,0.6,35,62,0,0,222.8,306.7,0,581.02897 01/09/2015 07:00,-0.2,0.3,270,65,0,0,210,305.5,0,581.02897 01/09/2015 08:00,0,0.6,52,59,114,23,218.3,312.3,0,581.02897 01/09/2015 09:00,1.9,1.5,176,57,173,35,220.2,322.8,0,581.02897 01/09/2015 10:00,3.4,1.9,183,47,331,67,245.8,372.6,0,581.02897 This file was automatically generated via lazydocs .","title":"function to_geotop"},{"location":"TopoPyScale.topo_obs/","text":"module TopoPyScale.topo_obs Tools to download and compare Downsclaed timeseries to observation All observation in folder inputs/obs/ S. Filhol December 2021 function get_metno_obs get_metno_obs ( sources , voi , start_date , end_date , client_id = '97a0e2bc-a262-48fe-9dea-5e9c894e9328' ) Function to download observation data from MetNo FROST API (Norwegian Meteorological institute) Args sources (list): station code, e.g. 'SN25830' voi (list): variables to download start_date (str): starting date end_date (str): ending date client_id (str): FROST_API_CLIENTID Return: dataframe: all data combined together List of variable: https://frost.met.no/element table Find out about stations: https://seklima.met.no/ WARNING: Download max one year at the time to not reach max limit of data to download. TODO: convert df to xarray dataset with stn_id, time as coordinates function combine_metno_obs_to_xarray combine_metno_obs_to_xarray ( fnames = 'metno*.pckl' , path = 'inputs/obs/' ) Function to convert metno format to usable dataset Args: fnames (str pattern): pattern of the file to load path (str): path of the file to load Returns: dataset : dataset will all data organized in timeseries and station number function fetch_WMO_insitu_observations fetch_WMO_insitu_observations ( year , month , bbox , target_path = './inputs/observations' ) Function to download WMO in-situ data from land surface in-situ observations from Copernicus. https://cds.climate.copernicus.eu/cdsapp#!/dataset/insitu-observations-surface-land?tab=overview Args: year (str or list): year(s) to download month (str or list): month(s) to download bbox (list): bonding box in lat-lon [lat_south, lon_west, lat_north, lon_east] target (str): filename Returns: Store to disk the dataset as zip file TODO: - [x] test function - [ ] check if can download large extent at once or need multiple requests? - [x] save data in individual files for each stations. store either as csv or netcdf (better) function parse_WMO_insitu_observations parse_WMO_insitu_observations ( fname = None , file_pattern = 'inputs/observations/surf*subset_csv*.csv' , path = './inputs/observations' ) Function to parse WMO files formated from CDS database. parse in single station file Args: fname: file_pattern: path: Returns: This file was automatically generated via lazydocs .","title":"topo_obs"},{"location":"TopoPyScale.topo_obs/#module-topopyscaletopo_obs","text":"Tools to download and compare Downsclaed timeseries to observation All observation in folder inputs/obs/ S. Filhol December 2021","title":"module TopoPyScale.topo_obs"},{"location":"TopoPyScale.topo_obs/#function-get_metno_obs","text":"get_metno_obs ( sources , voi , start_date , end_date , client_id = '97a0e2bc-a262-48fe-9dea-5e9c894e9328' ) Function to download observation data from MetNo FROST API (Norwegian Meteorological institute) Args sources (list): station code, e.g. 'SN25830' voi (list): variables to download start_date (str): starting date end_date (str): ending date client_id (str): FROST_API_CLIENTID Return: dataframe: all data combined together List of variable: https://frost.met.no/element table Find out about stations: https://seklima.met.no/ WARNING: Download max one year at the time to not reach max limit of data to download. TODO: convert df to xarray dataset with stn_id, time as coordinates","title":"function get_metno_obs"},{"location":"TopoPyScale.topo_obs/#function-combine_metno_obs_to_xarray","text":"combine_metno_obs_to_xarray ( fnames = 'metno*.pckl' , path = 'inputs/obs/' ) Function to convert metno format to usable dataset Args: fnames (str pattern): pattern of the file to load path (str): path of the file to load Returns: dataset : dataset will all data organized in timeseries and station number","title":"function combine_metno_obs_to_xarray"},{"location":"TopoPyScale.topo_obs/#function-fetch_wmo_insitu_observations","text":"fetch_WMO_insitu_observations ( year , month , bbox , target_path = './inputs/observations' ) Function to download WMO in-situ data from land surface in-situ observations from Copernicus. https://cds.climate.copernicus.eu/cdsapp#!/dataset/insitu-observations-surface-land?tab=overview Args: year (str or list): year(s) to download month (str or list): month(s) to download bbox (list): bonding box in lat-lon [lat_south, lon_west, lat_north, lon_east] target (str): filename Returns: Store to disk the dataset as zip file TODO: - [x] test function - [ ] check if can download large extent at once or need multiple requests? - [x] save data in individual files for each stations. store either as csv or netcdf (better)","title":"function fetch_WMO_insitu_observations"},{"location":"TopoPyScale.topo_obs/#function-parse_wmo_insitu_observations","text":"parse_WMO_insitu_observations ( fname = None , file_pattern = 'inputs/observations/surf*subset_csv*.csv' , path = './inputs/observations' ) Function to parse WMO files formated from CDS database. parse in single station file Args: fname: file_pattern: path: Returns: This file was automatically generated via lazydocs .","title":"function parse_WMO_insitu_observations"},{"location":"TopoPyScale.topo_param/","text":"module TopoPyScale.topo_param Set of functions to work with DEMs S. Filhol, Oct 2021 TODO: an improvement could be to first copmute horizons, and then SVF to avoid computing horizon twice function convert_epsg_pts convert_epsg_pts ( xs , ys , epsg_src = 4326 , epsg_tgt = 3844 ) Simple function to convert a list fo poitn from one projection to another oen using PyProj Args: xs (array): 1D array with X-coordinate expressed in the source EPSG ys (array): 1D array with Y-coordinate expressed in the source EPSG epsg_src (int): source projection EPSG code epsg_tgt (int): target projection EPSG code Returns: array : Xs 1D arrays of the point coordinates expressed in the target projection array : Ys 1D arrays of the point coordinates expressed in the target projection function get_extent_latlon get_extent_latlon ( dem_file , epsg_src ) Function to extract DEM extent in Lat/Lon Args: dem_file (str): path to DEM file (GeoTiFF) epsg_src (int): EPSG projection code Returns: dict : extent in lat/lon, {latN, latS, lonW, lonE} function extract_pts_param extract_pts_param ( df_pts , ds_param , method = 'nearest' ) Function to sample DEM parameters for a list point. This is used as an alternative the the TopoSub method, to perform downscaling at selected locations (x,y) WARNING: the projection and coordiante system of the EDM and point coordinates MUST be the same! Args: df_pts (dataframe): list of points coordinates with coordiantes in (x,y). ds_param (dataset): dem parameters method (str): sampling method. Supported 'nearest', 'linear' interpolation, 'idw' interpolation (inverse-distance weighted) Returns: dataframe : df_pts updated with new columns ['elevation', 'slope', 'aspect', 'aspect_cos', 'aspect_sin', 'svf'] function compute_dem_param compute_dem_param ( dem_file ) Function to compute and derive DEM parameters: slope, aspect, sky view factor Args: dem_file (str): path to raster file (geotif). Raster must be in local cartesian coordinate system (e.g. UTM) Returns: dataset : x, y, elev, slope, aspect, svf function compute_horizon compute_horizon ( dem_file , azimuth_inc = 30 , num_threads = None ) Function to compute horizon angles for Args: dem_file (str): path and filename of the dem azimuth_inc (int): angle increment to compute horizons at, in Degrees [0-359] num_threads (int): number of threads to parallize on Returns: dataarray : all horizon angles for x,y,azimuth coordinates This file was automatically generated via lazydocs .","title":"topo_param"},{"location":"TopoPyScale.topo_param/#module-topopyscaletopo_param","text":"Set of functions to work with DEMs S. Filhol, Oct 2021 TODO: an improvement could be to first copmute horizons, and then SVF to avoid computing horizon twice","title":"module TopoPyScale.topo_param"},{"location":"TopoPyScale.topo_param/#function-convert_epsg_pts","text":"convert_epsg_pts ( xs , ys , epsg_src = 4326 , epsg_tgt = 3844 ) Simple function to convert a list fo poitn from one projection to another oen using PyProj Args: xs (array): 1D array with X-coordinate expressed in the source EPSG ys (array): 1D array with Y-coordinate expressed in the source EPSG epsg_src (int): source projection EPSG code epsg_tgt (int): target projection EPSG code Returns: array : Xs 1D arrays of the point coordinates expressed in the target projection array : Ys 1D arrays of the point coordinates expressed in the target projection","title":"function convert_epsg_pts"},{"location":"TopoPyScale.topo_param/#function-get_extent_latlon","text":"get_extent_latlon ( dem_file , epsg_src ) Function to extract DEM extent in Lat/Lon Args: dem_file (str): path to DEM file (GeoTiFF) epsg_src (int): EPSG projection code Returns: dict : extent in lat/lon, {latN, latS, lonW, lonE}","title":"function get_extent_latlon"},{"location":"TopoPyScale.topo_param/#function-extract_pts_param","text":"extract_pts_param ( df_pts , ds_param , method = 'nearest' ) Function to sample DEM parameters for a list point. This is used as an alternative the the TopoSub method, to perform downscaling at selected locations (x,y) WARNING: the projection and coordiante system of the EDM and point coordinates MUST be the same! Args: df_pts (dataframe): list of points coordinates with coordiantes in (x,y). ds_param (dataset): dem parameters method (str): sampling method. Supported 'nearest', 'linear' interpolation, 'idw' interpolation (inverse-distance weighted) Returns: dataframe : df_pts updated with new columns ['elevation', 'slope', 'aspect', 'aspect_cos', 'aspect_sin', 'svf']","title":"function extract_pts_param"},{"location":"TopoPyScale.topo_param/#function-compute_dem_param","text":"compute_dem_param ( dem_file ) Function to compute and derive DEM parameters: slope, aspect, sky view factor Args: dem_file (str): path to raster file (geotif). Raster must be in local cartesian coordinate system (e.g. UTM) Returns: dataset : x, y, elev, slope, aspect, svf","title":"function compute_dem_param"},{"location":"TopoPyScale.topo_param/#function-compute_horizon","text":"compute_horizon ( dem_file , azimuth_inc = 30 , num_threads = None ) Function to compute horizon angles for Args: dem_file (str): path and filename of the dem azimuth_inc (int): angle increment to compute horizons at, in Degrees [0-359] num_threads (int): number of threads to parallize on Returns: dataarray : all horizon angles for x,y,azimuth coordinates This file was automatically generated via lazydocs .","title":"function compute_horizon"},{"location":"TopoPyScale.topo_plot/","text":"module TopoPyScale.topo_plot Collectio of plotting functions for TopoPyScale S. Filhol, December 2021 function map__terrain map__terrain ( ds_param , var = 'elevation' , hillshade = True ) Function to plot terrain parameters Args: ds_param: var: hillshade: Returns: function map_unclustered map_unclustered ( ds_down , ds_param , time_step = 1 , var = 't' , cmap =< matplotlib . colors . LinearSegmentedColormap object at 0x7ffa9a410970 > , hillshade = True , ** kwargs ) Function to plot unclustered downscaled points given that each point corresponds to a cluster label Args: ds_down: ds_param: time_step: var: cmap: - **kwargs : kwargs to pass to imshow() in dict format. TODO: if temperature, divergent colorscale around 0degC see if we can remain within xarray dataset/dataarray to reshape and include x,y This file was automatically generated via lazydocs .","title":"topo_plot"},{"location":"TopoPyScale.topo_plot/#module-topopyscaletopo_plot","text":"Collectio of plotting functions for TopoPyScale S. Filhol, December 2021","title":"module TopoPyScale.topo_plot"},{"location":"TopoPyScale.topo_plot/#function-map__terrain","text":"map__terrain ( ds_param , var = 'elevation' , hillshade = True ) Function to plot terrain parameters Args: ds_param: var: hillshade: Returns:","title":"function map__terrain"},{"location":"TopoPyScale.topo_plot/#function-map_unclustered","text":"map_unclustered ( ds_down , ds_param , time_step = 1 , var = 't' , cmap =< matplotlib . colors . LinearSegmentedColormap object at 0x7ffa9a410970 > , hillshade = True , ** kwargs ) Function to plot unclustered downscaled points given that each point corresponds to a cluster label Args: ds_down: ds_param: time_step: var: cmap: - **kwargs : kwargs to pass to imshow() in dict format. TODO: if temperature, divergent colorscale around 0degC see if we can remain within xarray dataset/dataarray to reshape and include x,y This file was automatically generated via lazydocs .","title":"function map_unclustered"},{"location":"TopoPyScale.topo_scale/","text":"module TopoPyScale.topo_scale Toposcale functionalities S. Filhol, Oct 2021 ======= Organization of input data to Toposcale ====== dem_description (nb_points) X Y, elev, slope, aspect, svf, x_ERA, y_ERA, elev_ERA, horizon_angles (nb_points * bins) solar_geom (nb_points * time * 2) surface_climate_data (spatial + time + var) plevels_climate_data (spatial + time + var + plevel) ======= Dataset naming convention: ========= ds_surf => dataset direct from ERA5 single level surface ds_plev => dataset direct from ERA5 pressure levels ds_surf_pt => 3 3 grid ERA5 single surface level around a given point (lat,lon) ds_plev_pt => 3 3 grid ERA5 pressure level around a given point (lat,lon) plev_interp => horizontal interpolation of the 3 3 grid at the point (lat,lon) for the pressure levels surf_interp => horizontal interpolation of the 3 3 grid at the point (lat,lon) for the single surface level top => closest pressure level above the point (lat,lon,elev) for each timesep bot => closest pressure level below the point (lat,lon,elev) for each timesep down_pt => downscaled data time series (t, u, v, q, LW, SW, tp) TODO: check that transformation from r,t to q for plevels is working fine add metadata to all newly created variables in datasets ds_psurf, ds_plevel, down_pt upscale method for all points in df_centroids: method (1) simply using a for loop over each row method (2) concatenate 3*3 grid along an additional dimension to process all points in one go. Global Variables g R function downscale_climate downscale_climate ( path_forcing , df_centroids , horizon_da , target_EPSG , start_date , end_date , interp_method = 'idw' , lw_terrain_flag = True , tstep = '1H' ) Function to perform downscaling of climate variables (t,q,u,v,tp,SW,LW) based on Toposcale logic Args: path_forcing : path to forcing data [SURF , PLEV ] df_centroids (dataframe): containing a list of point for which to downscale (includes all terrain data) horizon_da (dataarray): horizon angles for a list of azimuth target_EPSG (int): EPSG code of the DEM interp_method (str): interpolation method for horizontal interp. 'idw' or 'linear' lw_terrain_flag (bool): flag to compute contribution of surrounding terrain to LW or ignore tstep (str): timestep of the input data, default = 1H Returns: dataset : downscaled data organized with time, point_id, lat, long function read_downscaled read_downscaled ( path = 'outputs/down_pt*.nc' ) Function to read downscaled points saved into netcdf files into one single dataset using Dask Args: path (str): path and pattern to use into xr.open_mfdataset Returns: dataset : merged dataset readily to use and loaded in chuncks via Dask This file was automatically generated via lazydocs .","title":"topo_scale"},{"location":"TopoPyScale.topo_scale/#module-topopyscaletopo_scale","text":"Toposcale functionalities S. Filhol, Oct 2021 ======= Organization of input data to Toposcale ====== dem_description (nb_points) X Y, elev, slope, aspect, svf, x_ERA, y_ERA, elev_ERA, horizon_angles (nb_points * bins) solar_geom (nb_points * time * 2) surface_climate_data (spatial + time + var) plevels_climate_data (spatial + time + var + plevel) ======= Dataset naming convention: ========= ds_surf => dataset direct from ERA5 single level surface ds_plev => dataset direct from ERA5 pressure levels ds_surf_pt => 3 3 grid ERA5 single surface level around a given point (lat,lon) ds_plev_pt => 3 3 grid ERA5 pressure level around a given point (lat,lon) plev_interp => horizontal interpolation of the 3 3 grid at the point (lat,lon) for the pressure levels surf_interp => horizontal interpolation of the 3 3 grid at the point (lat,lon) for the single surface level top => closest pressure level above the point (lat,lon,elev) for each timesep bot => closest pressure level below the point (lat,lon,elev) for each timesep down_pt => downscaled data time series (t, u, v, q, LW, SW, tp) TODO: check that transformation from r,t to q for plevels is working fine add metadata to all newly created variables in datasets ds_psurf, ds_plevel, down_pt upscale method for all points in df_centroids: method (1) simply using a for loop over each row method (2) concatenate 3*3 grid along an additional dimension to process all points in one go.","title":"module TopoPyScale.topo_scale"},{"location":"TopoPyScale.topo_scale/#global-variables","text":"g R","title":"Global Variables"},{"location":"TopoPyScale.topo_scale/#function-downscale_climate","text":"downscale_climate ( path_forcing , df_centroids , horizon_da , target_EPSG , start_date , end_date , interp_method = 'idw' , lw_terrain_flag = True , tstep = '1H' ) Function to perform downscaling of climate variables (t,q,u,v,tp,SW,LW) based on Toposcale logic Args: path_forcing : path to forcing data [SURF , PLEV ] df_centroids (dataframe): containing a list of point for which to downscale (includes all terrain data) horizon_da (dataarray): horizon angles for a list of azimuth target_EPSG (int): EPSG code of the DEM interp_method (str): interpolation method for horizontal interp. 'idw' or 'linear' lw_terrain_flag (bool): flag to compute contribution of surrounding terrain to LW or ignore tstep (str): timestep of the input data, default = 1H Returns: dataset : downscaled data organized with time, point_id, lat, long","title":"function downscale_climate"},{"location":"TopoPyScale.topo_scale/#function-read_downscaled","text":"read_downscaled ( path = 'outputs/down_pt*.nc' ) Function to read downscaled points saved into netcdf files into one single dataset using Dask Args: path (str): path and pattern to use into xr.open_mfdataset Returns: dataset : merged dataset readily to use and loaded in chuncks via Dask This file was automatically generated via lazydocs .","title":"function read_downscaled"},{"location":"TopoPyScale.topo_sim/","text":"module TopoPyScale.topo_sim Methods to generate required simulation files and run simulations of various models using tscale forcing J. Fiddes, February 2022 TODO: function fsm_nlst fsm_nlst ( nconfig , metfile , nave ) Function to generate namelist parameter file that is required to run the FSM model. https://github.com/RichardEssery/FSM Args: nconfig (int): which FSm configuration to run (integer 1-31) metfile (str): path to input tscale file (relative as Fortran fails with long strings (max 21 chars?)) nave (int): number of forcing steps to average output over eg if forcing is hourly and output required is daily then nave = 24 Returns: NULL (writes out namelist text file which configures a single FSM run) Notes: constraint is that Fortran fails with long strings (max?) definition: https://github.com/RichardEssery/FSM/blob/master/nlst_CdP_0506.txt Example nlst: &config / &drive met_file = 'data/met_CdP_0506.txt' zT = 1.5 zvar = .FALSE. / &params / &initial Tsoil = 282.98 284.17 284.70 284.70 / &outputs out_file = 'out_CdP_0506.txt' / function fsm_sim fsm_sim ( nlstfile , fsm_exec ) Function to simulate the FSM model https://github.com/RichardEssery/FSM Args: nlstfile (int): which FSm configuration to run (integer 1-31) fsm_exec (str): path to input tscale file (relative as Fortran fails with long strings (max 21 chars?)) Returns: NULL (FSM simulation file written to disk) function agg_by_var_fsm agg_by_var_fsm ( ncol ) Function to make single variable multi cluster files as preprocessing step before spatialisation. This is much more efficient than looping over individual simulation files per cluster. For V variables , C clusters and T timesteps this turns C individual files of dimensions V x T into V individual files of dimensions C x T. Currently written for FSM files but could be generalised to other models. Args: ncol (int): column number of variable to extract Returns: NULL ( file written to disk) ncol: 4 = rof 5 = hs 6 = swe 7 = gst function agg_by_var_fsm_ensemble agg_by_var_fsm_ensemble ( ncol , W ) Function to make single variable multi cluster files as preprocessing step before spatialisation. This is much more efficient than looping over individual simulation files per cluster. For V variables , C clusters and T timesteps this turns C individual files of dimensions V x T into V individual files of dimensions C x T. Currently written for FSM files but could be generalised to other models. Args: ncol (int): column number of variable to extract Returns: NULL ( file written to disk) ncol: 4 = rof 5 = hs 6 = swe 7 = gst function timeseries_means_period timeseries_means_period ( df , start_date , end_date ) Function to extract results vectors from simulation results. This can be entire time period some subset or sing day. Args: df (dataframe): results df start_date (str): start date of average (can be used to extract single day) '2020-01-01' end_date (str): end date of average (can be used to extract single day) '2020-01-03' Returns: dataframe : averaged dataframe function topo_map topo_map ( df_mean , outname = 'outputmap.tif' ) Function to map results to toposub clusters generating map results. Args: df_mean (dataframe): an array of values to map to dem same length as number of toposub clusters Here 's an approach for arbitrary reclassification of integer rasters that avoids using a million calls to np.where. Rasterio bits taken from @Aaron' s answer: https://gis.stackexchange.com/questions/163007/raster-reclassify-using-python-gdal-and-numpy function topo_map_forcing topo_map_forcing ( ds_var , round_dp , mydtype , new_res = None ) Function to map forcing to toposub clusters generating gridded forcings Args: ds_var : single variable of ds eg. mp.downscaled_pts.t new_res : optional parameter to resample output to (in units of projection Return: - grid_stack : stack of grids with dimension Time x Y x X Here 's an approach for arbitrary reclassification of integer rasters that avoids using a million calls to np.where. Rasterio bits taken from @Aaron' s answer: https://gis.stackexchange.com/questions/163007/raster-reclassify-using-python-gdal-and-numpy function write_ncdf write_ncdf ( wdir , grid_stack , var , units , longname , mytime , lats , lons , mydtype ) This file was automatically generated via lazydocs .","title":"topo_sim"},{"location":"TopoPyScale.topo_sim/#module-topopyscaletopo_sim","text":"Methods to generate required simulation files and run simulations of various models using tscale forcing J. Fiddes, February 2022 TODO:","title":"module TopoPyScale.topo_sim"},{"location":"TopoPyScale.topo_sim/#function-fsm_nlst","text":"fsm_nlst ( nconfig , metfile , nave ) Function to generate namelist parameter file that is required to run the FSM model. https://github.com/RichardEssery/FSM Args: nconfig (int): which FSm configuration to run (integer 1-31) metfile (str): path to input tscale file (relative as Fortran fails with long strings (max 21 chars?)) nave (int): number of forcing steps to average output over eg if forcing is hourly and output required is daily then nave = 24 Returns: NULL (writes out namelist text file which configures a single FSM run) Notes: constraint is that Fortran fails with long strings (max?) definition: https://github.com/RichardEssery/FSM/blob/master/nlst_CdP_0506.txt Example nlst: &config / &drive met_file = 'data/met_CdP_0506.txt' zT = 1.5 zvar = .FALSE. / &params / &initial Tsoil = 282.98 284.17 284.70 284.70 / &outputs out_file = 'out_CdP_0506.txt' /","title":"function fsm_nlst"},{"location":"TopoPyScale.topo_sim/#function-fsm_sim","text":"fsm_sim ( nlstfile , fsm_exec ) Function to simulate the FSM model https://github.com/RichardEssery/FSM Args: nlstfile (int): which FSm configuration to run (integer 1-31) fsm_exec (str): path to input tscale file (relative as Fortran fails with long strings (max 21 chars?)) Returns: NULL (FSM simulation file written to disk)","title":"function fsm_sim"},{"location":"TopoPyScale.topo_sim/#function-agg_by_var_fsm","text":"agg_by_var_fsm ( ncol ) Function to make single variable multi cluster files as preprocessing step before spatialisation. This is much more efficient than looping over individual simulation files per cluster. For V variables , C clusters and T timesteps this turns C individual files of dimensions V x T into V individual files of dimensions C x T. Currently written for FSM files but could be generalised to other models. Args: ncol (int): column number of variable to extract Returns: NULL ( file written to disk) ncol: 4 = rof 5 = hs 6 = swe 7 = gst","title":"function agg_by_var_fsm"},{"location":"TopoPyScale.topo_sim/#function-agg_by_var_fsm_ensemble","text":"agg_by_var_fsm_ensemble ( ncol , W ) Function to make single variable multi cluster files as preprocessing step before spatialisation. This is much more efficient than looping over individual simulation files per cluster. For V variables , C clusters and T timesteps this turns C individual files of dimensions V x T into V individual files of dimensions C x T. Currently written for FSM files but could be generalised to other models. Args: ncol (int): column number of variable to extract Returns: NULL ( file written to disk) ncol: 4 = rof 5 = hs 6 = swe 7 = gst","title":"function agg_by_var_fsm_ensemble"},{"location":"TopoPyScale.topo_sim/#function-timeseries_means_period","text":"timeseries_means_period ( df , start_date , end_date ) Function to extract results vectors from simulation results. This can be entire time period some subset or sing day. Args: df (dataframe): results df start_date (str): start date of average (can be used to extract single day) '2020-01-01' end_date (str): end date of average (can be used to extract single day) '2020-01-03' Returns: dataframe : averaged dataframe","title":"function timeseries_means_period"},{"location":"TopoPyScale.topo_sim/#function-topo_map","text":"topo_map ( df_mean , outname = 'outputmap.tif' ) Function to map results to toposub clusters generating map results. Args: df_mean (dataframe): an array of values to map to dem same length as number of toposub clusters Here 's an approach for arbitrary reclassification of integer rasters that avoids using a million calls to np.where. Rasterio bits taken from @Aaron' s answer: https://gis.stackexchange.com/questions/163007/raster-reclassify-using-python-gdal-and-numpy","title":"function topo_map"},{"location":"TopoPyScale.topo_sim/#function-topo_map_forcing","text":"topo_map_forcing ( ds_var , round_dp , mydtype , new_res = None ) Function to map forcing to toposub clusters generating gridded forcings Args: ds_var : single variable of ds eg. mp.downscaled_pts.t new_res : optional parameter to resample output to (in units of projection Return: - grid_stack : stack of grids with dimension Time x Y x X Here 's an approach for arbitrary reclassification of integer rasters that avoids using a million calls to np.where. Rasterio bits taken from @Aaron' s answer: https://gis.stackexchange.com/questions/163007/raster-reclassify-using-python-gdal-and-numpy","title":"function topo_map_forcing"},{"location":"TopoPyScale.topo_sim/#function-write_ncdf","text":"write_ncdf ( wdir , grid_stack , var , units , longname , mytime , lats , lons , mydtype ) This file was automatically generated via lazydocs .","title":"function write_ncdf"},{"location":"TopoPyScale.topo_sub/","text":"module TopoPyScale.topo_sub Clustering routines for TopoSUB S. Filhol, Oct 2021 TODO: explore other clustering methods available in scikit-learn: https://scikit-learn.org/stable/modules/clustering.html look into DBSCAN and its relative function ds_to_indexed_dataframe ds_to_indexed_dataframe ( ds ) Function to convert an Xarray dataset with multi-dimensions to indexed dataframe (and not a multilevel indexed dataframe). WARNING: this only works if the variable of the dataset have all the same dimensions! By default the ds.to_dataframe() returns a multi-index dataframe. Here the coordinates are transfered as columns in the dataframe Args: ds (dataset): xarray dataset with all variable of same number of dimensions Returns: pandas dataframe: function scale_df scale_df ( df_param , scaler = StandardScaler ()) Function to scale features of a pandas dataframe Args: df_param (dataframe): features to scale scaler (scaler object): Default is StandardScaler() Returns: dataframe : scaled data function inverse_scale_df inverse_scale_df ( df_scaled , scaler ) Function to inverse feature scaling of a pandas dataframe Args: df_scaled (dataframe): scaled data to transform back to original (inverse transfrom) scaler (scaler object): original scikit learn scaler Returns: dataframe : data in original format function kmeans_clustering kmeans_clustering ( df_param , n_clusters = 100 , seed = None , ** kwargs ) Function to perform K-mean clustering Args: df_param (dataframe): features n_clusters (int): number of clusters seed (int): None or int for random seed generator kwargs: Returns: dataframe : df_centers kmean object : kmeans dataframe : df_param function minibatch_kmeans_clustering minibatch_kmeans_clustering ( df_param , n_clusters = 100 , n_cores = 4 , seed = None , ** kwargs ) Function to perform mini-batch K-mean clustering Args: df_param (dataframe): features n_clusters (int): number of clusters n_cores (int): number of processor core kwargs: Returns: dataframe : centroids kmean object : kmean model dataframe : labels of input data function plot_center_clusters plot_center_clusters ( dem_file , ds_param , df_centers , var = 'elevation' , cmap =< matplotlib . colors . ListedColormap object at 0x7ffa9a5dd340 > , figsize = ( 14 , 10 ) ) Function to plot the location of the cluster centroids over the DEM Args: dem_file (str): path to dem raster file ds_param (dataset): topo_param parameters ['elev', 'slope', 'aspect_cos', 'aspect_sin', 'svf'] df_centers (dataframe): containing cluster centroid parameters ['x', 'y', 'elev', 'slope', 'aspect_cos', 'aspect_sin', 'svf'] var (str): variable to plot as background cmap (pyplot cmap): pyplot colormap to represent the variable. function plot_pca_clusters plot_pca_clusters ( dem_file , df_param , df_centroids , scaler , n_components , subsample = 3 ) function write_landform write_landform ( dem_file , df_param ) Function to write a landform file which maps cluster ids to dem pixels Args: dem_file (str): path to dem raster file ds_param (dataset): topo_param parameters ['elev', 'slope', 'aspect_cos', 'aspect_sin', 'svf'] This file was automatically generated via lazydocs .","title":"topo_sub"},{"location":"TopoPyScale.topo_sub/#module-topopyscaletopo_sub","text":"Clustering routines for TopoSUB S. Filhol, Oct 2021 TODO: explore other clustering methods available in scikit-learn: https://scikit-learn.org/stable/modules/clustering.html look into DBSCAN and its relative","title":"module TopoPyScale.topo_sub"},{"location":"TopoPyScale.topo_sub/#function-ds_to_indexed_dataframe","text":"ds_to_indexed_dataframe ( ds ) Function to convert an Xarray dataset with multi-dimensions to indexed dataframe (and not a multilevel indexed dataframe). WARNING: this only works if the variable of the dataset have all the same dimensions! By default the ds.to_dataframe() returns a multi-index dataframe. Here the coordinates are transfered as columns in the dataframe Args: ds (dataset): xarray dataset with all variable of same number of dimensions Returns: pandas dataframe:","title":"function ds_to_indexed_dataframe"},{"location":"TopoPyScale.topo_sub/#function-scale_df","text":"scale_df ( df_param , scaler = StandardScaler ()) Function to scale features of a pandas dataframe Args: df_param (dataframe): features to scale scaler (scaler object): Default is StandardScaler() Returns: dataframe : scaled data","title":"function scale_df"},{"location":"TopoPyScale.topo_sub/#function-inverse_scale_df","text":"inverse_scale_df ( df_scaled , scaler ) Function to inverse feature scaling of a pandas dataframe Args: df_scaled (dataframe): scaled data to transform back to original (inverse transfrom) scaler (scaler object): original scikit learn scaler Returns: dataframe : data in original format","title":"function inverse_scale_df"},{"location":"TopoPyScale.topo_sub/#function-kmeans_clustering","text":"kmeans_clustering ( df_param , n_clusters = 100 , seed = None , ** kwargs ) Function to perform K-mean clustering Args: df_param (dataframe): features n_clusters (int): number of clusters seed (int): None or int for random seed generator kwargs: Returns: dataframe : df_centers kmean object : kmeans dataframe : df_param","title":"function kmeans_clustering"},{"location":"TopoPyScale.topo_sub/#function-minibatch_kmeans_clustering","text":"minibatch_kmeans_clustering ( df_param , n_clusters = 100 , n_cores = 4 , seed = None , ** kwargs ) Function to perform mini-batch K-mean clustering Args: df_param (dataframe): features n_clusters (int): number of clusters n_cores (int): number of processor core kwargs: Returns: dataframe : centroids kmean object : kmean model dataframe : labels of input data","title":"function minibatch_kmeans_clustering"},{"location":"TopoPyScale.topo_sub/#function-plot_center_clusters","text":"plot_center_clusters ( dem_file , ds_param , df_centers , var = 'elevation' , cmap =< matplotlib . colors . ListedColormap object at 0x7ffa9a5dd340 > , figsize = ( 14 , 10 ) ) Function to plot the location of the cluster centroids over the DEM Args: dem_file (str): path to dem raster file ds_param (dataset): topo_param parameters ['elev', 'slope', 'aspect_cos', 'aspect_sin', 'svf'] df_centers (dataframe): containing cluster centroid parameters ['x', 'y', 'elev', 'slope', 'aspect_cos', 'aspect_sin', 'svf'] var (str): variable to plot as background cmap (pyplot cmap): pyplot colormap to represent the variable.","title":"function plot_center_clusters"},{"location":"TopoPyScale.topo_sub/#function-plot_pca_clusters","text":"plot_pca_clusters ( dem_file , df_param , df_centroids , scaler , n_components , subsample = 3 )","title":"function plot_pca_clusters"},{"location":"TopoPyScale.topo_sub/#function-write_landform","text":"write_landform ( dem_file , df_param ) Function to write a landform file which maps cluster ids to dem pixels Args: dem_file (str): path to dem raster file ds_param (dataset): topo_param parameters ['elev', 'slope', 'aspect_cos', 'aspect_sin', 'svf'] This file was automatically generated via lazydocs .","title":"function write_landform"},{"location":"TopoPyScale.topoclass/","text":"module TopoPyScale.topoclass Toposcale class definition S. Filhol, September 2021 project/ config.ini -> input/ -> dem/ -> climate/ -> output/ class Topoclass A python class to bring the typical use-case of toposcale in a user friendly object method __init__ __init__ ( config_file ) method compute_dem_param compute_dem_param () method compute_horizon compute_horizon () Function to compute horizon angle and sample values for list of points :return: method compute_solar_geometry compute_solar_geometry () method downscale_climate downscale_climate () method extract_dem_cluster_param extract_dem_cluster_param () Function to segment a DEM in clusters and retain only the centroids of each cluster. :return: method extract_pts_param extract_pts_param ( method = 'nearest' , ** kwargs ) Function to use a list point as input rather than cluster centroids from DEM segmentation (topo_sub.py/self.clustering_dem()). Args: df (dataFrame): - method (str): method of sampling - **kwargs : pd.read_csv() parameters Returns: method extract_topo_param extract_topo_param () Function to select which method get_WMO_observations get_WMO_observations () Function to download and parse in-situ data from WMO database method get_era5 get_era5 () Funtion to call fetching of ERA5 data TODO: merge monthly data into one file (cdo?)- this creates massive slow down! method to_crocus to_crocus ( fname_format = './outputs/CROCUS_pt_*.nc' , scale_precip = 1 ) function to export toposcale output to crocus format .nc. This functions saves one file per point_id Args: fout_format (str): filename format. point_id is inserted where * is scale_precip (float): scaling factor to apply on precipitation. Default is 1 method to_cryogrid to_cryogrid ( fname_format = 'Cryogrid_pt_*.nc' , precip_partition = 'continuous' ) wrapper function to export toposcale output to cryosgrid format from TopoClass Args: fname_format (str): filename format. point_id is inserted where * is method to_fsm to_fsm ( fname_format = './outputs/FSM_pt_*.txt' ) function to export toposcale output to FSM format method to_geotop to_geotop ( fname_format = './outputs/meteo_*.txt' ) function to export toposcale output to FSM format method to_musa to_musa ( fname_met = 'musa_met.nc' , fname_labels = 'musa_labels.nc' ) function to export TopoPyScale output in a format compatible with MuSa MuSa: https://github.com/ealonsogzl/MuSA Args: fname : filename of the netcdf method to_netcdf to_netcdf ( file_out = './outputs/output.nc' ) function to export toposcale output to one single generic netcdf format, compressed Args: file_out (str): name of export file method to_snowmodel to_snowmodel ( fname_format = './outputs/Snowmodel_stn_*.csv' ) function to export toposcale output to snowmodel format .ascii, for single station standard fout_format: str, filename format. point_id is inserted where * is method to_snowpack to_snowpack ( fname_format = './outputs/smet_pt_*.smet' ) function to export toposcale output to FSM format This file was automatically generated via lazydocs .","title":"topoclass"},{"location":"TopoPyScale.topoclass/#module-topopyscaletopoclass","text":"Toposcale class definition S. Filhol, September 2021 project/ config.ini -> input/ -> dem/ -> climate/ -> output/","title":"module TopoPyScale.topoclass"},{"location":"TopoPyScale.topoclass/#class-topoclass","text":"A python class to bring the typical use-case of toposcale in a user friendly object","title":"class Topoclass"},{"location":"TopoPyScale.topoclass/#method-__init__","text":"__init__ ( config_file )","title":"method __init__"},{"location":"TopoPyScale.topoclass/#method-compute_dem_param","text":"compute_dem_param ()","title":"method compute_dem_param"},{"location":"TopoPyScale.topoclass/#method-compute_horizon","text":"compute_horizon () Function to compute horizon angle and sample values for list of points :return:","title":"method compute_horizon"},{"location":"TopoPyScale.topoclass/#method-compute_solar_geometry","text":"compute_solar_geometry ()","title":"method compute_solar_geometry"},{"location":"TopoPyScale.topoclass/#method-downscale_climate","text":"downscale_climate ()","title":"method downscale_climate"},{"location":"TopoPyScale.topoclass/#method-extract_dem_cluster_param","text":"extract_dem_cluster_param () Function to segment a DEM in clusters and retain only the centroids of each cluster. :return:","title":"method extract_dem_cluster_param"},{"location":"TopoPyScale.topoclass/#method-extract_pts_param","text":"extract_pts_param ( method = 'nearest' , ** kwargs ) Function to use a list point as input rather than cluster centroids from DEM segmentation (topo_sub.py/self.clustering_dem()). Args: df (dataFrame): - method (str): method of sampling - **kwargs : pd.read_csv() parameters Returns:","title":"method extract_pts_param"},{"location":"TopoPyScale.topoclass/#method-extract_topo_param","text":"extract_topo_param () Function to select which","title":"method extract_topo_param"},{"location":"TopoPyScale.topoclass/#method-get_wmo_observations","text":"get_WMO_observations () Function to download and parse in-situ data from WMO database","title":"method get_WMO_observations"},{"location":"TopoPyScale.topoclass/#method-get_era5","text":"get_era5 () Funtion to call fetching of ERA5 data TODO: merge monthly data into one file (cdo?)- this creates massive slow down!","title":"method get_era5"},{"location":"TopoPyScale.topoclass/#method-to_crocus","text":"to_crocus ( fname_format = './outputs/CROCUS_pt_*.nc' , scale_precip = 1 ) function to export toposcale output to crocus format .nc. This functions saves one file per point_id Args: fout_format (str): filename format. point_id is inserted where * is scale_precip (float): scaling factor to apply on precipitation. Default is 1","title":"method to_crocus"},{"location":"TopoPyScale.topoclass/#method-to_cryogrid","text":"to_cryogrid ( fname_format = 'Cryogrid_pt_*.nc' , precip_partition = 'continuous' ) wrapper function to export toposcale output to cryosgrid format from TopoClass Args: fname_format (str): filename format. point_id is inserted where * is","title":"method to_cryogrid"},{"location":"TopoPyScale.topoclass/#method-to_fsm","text":"to_fsm ( fname_format = './outputs/FSM_pt_*.txt' ) function to export toposcale output to FSM format","title":"method to_fsm"},{"location":"TopoPyScale.topoclass/#method-to_geotop","text":"to_geotop ( fname_format = './outputs/meteo_*.txt' ) function to export toposcale output to FSM format","title":"method to_geotop"},{"location":"TopoPyScale.topoclass/#method-to_musa","text":"to_musa ( fname_met = 'musa_met.nc' , fname_labels = 'musa_labels.nc' ) function to export TopoPyScale output in a format compatible with MuSa MuSa: https://github.com/ealonsogzl/MuSA Args: fname : filename of the netcdf","title":"method to_musa"},{"location":"TopoPyScale.topoclass/#method-to_netcdf","text":"to_netcdf ( file_out = './outputs/output.nc' ) function to export toposcale output to one single generic netcdf format, compressed Args: file_out (str): name of export file","title":"method to_netcdf"},{"location":"TopoPyScale.topoclass/#method-to_snowmodel","text":"to_snowmodel ( fname_format = './outputs/Snowmodel_stn_*.csv' ) function to export toposcale output to snowmodel format .ascii, for single station standard fout_format: str, filename format. point_id is inserted where * is","title":"method to_snowmodel"},{"location":"TopoPyScale.topoclass/#method-to_snowpack","text":"to_snowpack ( fname_format = './outputs/smet_pt_*.smet' ) function to export toposcale output to FSM format This file was automatically generated via lazydocs .","title":"method to_snowpack"}]}